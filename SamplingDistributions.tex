\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Sampling Distributions}
\author{Naveen Thakur }
\date{March 2021}

\begin{document}

\maketitle

\section{Introduction}
A 'sample statistic' is calculated using data from a sample. We're going to discuss how these statistics can be generalised to infer something about population parameters. To aid the discussion we focus on the simple statistic of 'the mean' as this unlocks one of statistics mighty pillars.

Let's start with a contrast. In regression analysis we take a sample and then analyse the relationship between two variables. This is very much focused upon the sample itself. However, in statistical analysis we want to go beyond talking about attributes of a single sample. We want to understand what those attributes say about the population from which the sample was drawn.

This jump from particular sample to general population is called statistical inference. But how is that jump made? What is hidden within a single sample that might help us infer something about its wider population? In fact, the answer lies within the study of \textbf{many} samples. Imagine drawing a random sample of fixed size from the population. It is a 'random sample' because each member of the population has an equal chance of being selected. Now record the mean of that sample and repeat the process of drawing a sample and recording the mean a large number of times. Statisticians then group these means to form a theoretical construct known as a sampling distribution. It is a distribution because each of these means will differ to some degree as each sample is more or less different from the other.

As an aside, do not confuse 'sampling distribution' with a 'sample distribution': the 'sampling' describes the shape of the mean when sampling repeatedly, while the 'sample' concerns the shape of the data within a single sample.

And it is within the sampling distributions that statisticians have discovered amazing regularities which act like a bridge from the particular to the general upon which inference can pass. These regularities are captured by the 'central limit theorem'.

The central limit theorem says that if you take a large number of samples of fixed size then the distribution of the means will approach a normal distribution. Further, the mean of the sampling distribution approximates the mean of the population from which the samples were drawn. Finally, the standard deviation of the sampling distribution (of the sample means) is equal to the population standard deviation divided by the square root of the fixed sample size.

The theorem has been shown to be true in a wide range of circumstances and is a central engine of statistical inference. It is these regularities that connect characteristics of individual samples to population parameters.

\subsection{Sample Error and Standard Error}

Earlier we explained why the mean differed between random samples. The 'sample error' or 'sample variation' (statisticians often uses the term 'error' to indicate 'noise' or 'variation' and not 'error' as in mistake) describes the degree to which a particular sample mean deviates from the population mean. Given the CLT, we expect these sample errors to vary in magnitude and resemble a normal distribution. The standard deviation of these errors is renamed 'standard error'. Therefore, the standard error gives an indication of how spread out are the magnitudes of the sample errors.

\subsection{CLT in action}
Quite often in the previous discussion we have spoken about the population parameter as though it's known which begs the question 'If you already know the population parameter - there isn't much left to infer?!'. 

Of course, we often don't know the population mean. But we can hypothesize its value and test a sample mean to see whether that value is so unusual that the hypothesized population mean looks ludicrous. We can do this because given a hypothesized mean, the CLT tells us that a distribution of the sample mean approximates a normal distribution. And it is this distribution that allows us to work out exactly how (un)likely we were to get the actual sample in our hands if the hypothesized mean were true. 





\end{document}
