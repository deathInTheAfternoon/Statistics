\documentclass[a4paper,twosided,notoc]{tufte-book}
\usepackage[utf8]{inputenc}

\hypersetup{colorlinks}	

%metadata for book
\author{Naveen Thakur}
\title{Statistics in Action}
\date{January 2021}
\publisher{Version 0.1}

\begin{document}
\maketitle
\begin{fullwidth}
	\tableofcontents
\end{fullwidth}


\chapter{Statistical Inference}
A 'sample statistic' such as the mean, variance or proportion is calculated using data obtained from an actual sample \sidenote{What is the mean of 0.12, 1.1, 0.9, 0.17, 0.08? These are the masses relative to the sun of the five nearest star-like objects in space. The summary data (statistic) is the requested mean and the list of solar masses the sample data.}
We're going to discuss how these statistics can be generalised to infer something about the population from which it came. Why is this important? Here's an example.

In 2014, American and European surveys showed that between $15\%-30\%$ of adult workers are involved in shift work \sidenote{\href{https://www.sciencedirect.com/science/article/abs/pii/S0369811414001230}{Impact of shift work on sleep and circadian rhythms}}. The survey paper looked specifically at the potentially dangerous effects of shift work on worker performance. However, by 2020 research \sidenote{\href{https://www.nature.com/articles/s41597-020-00709-6}{Brain activity and transcriptional profiling in mice under chronic jet lag}}is suggesting that shift work may be associated with far more types of 'adverse health outcomes' including increased risk of cardiovascular disorders, neurological diseases, cognitive defects and increased risk of mental illnesses.

Of course, the question is 'why'? Why is shift work affecting the brain/body and increasing the risk of various conditions? The title of the second paper mentioned above includes the intriguing phrase '...mice under chronic jet lag (CJL)'. It turns out that you can simulate shift work by inducing CJL in mice, by disrupting their exposure to light and dark. PET scans can then be used to examine the volume of activity within their brains (measured by glucose usage) and RNA sequencing used to show any disruption in gene expression that may be causing glucose changes. The results are startling in that after 34 days there are 'prominent and pervasive' changes in brain activity and that even after 10 days circadian gene expression \sidenote{\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3762864/}{Beautiful definition of circadian rhythms which affect body and brain:} "As the sun sets, nocturnal rodents begin to forage, nocturnal birds of prey begin their hunt while diurnal birds of prey sleep, filamentous fungi begin their daily production of spores, and cyanobacteria begin nitrogen fixation in an environment of low O2 after the day’s photosynthesis. As the sun rises the next morning many plants have positioned their leaves to catch the first rays of light and many humans sit motionless in cars on a nearby gridlocked highway...the obedience to temporal niches in all organisms is governed by a molecular circadian clock...not driven by sunlight, but synchronized by the 24 hour patterns of light and temperature produced by the earth’s rotation."} undergoes substantial change. 

But our question is very specific: only 15 mice were used to test the effect of CJL. But there are reportedly 10s of millions \sidenote{That's a  \href{https://www.sciencemag.org/news/2021/01/how-many-mice-and-rats-are-used-us-labs-controversial-study-says-more-100-million}{gory and statistically controversial number} in an area of ethical minefields. 6 mice were sacrificed in this study - what's your evaluation?} of mice in laboratories being used for research as you read this. Surely we should test all those mice to be sure?! It's obvious why each mouse in the world was not subject to CJL, PET, brain dissection and RNA sequencing. The time and cost would be prohibitive and a world without mice unthinkable. So why are we entitled to generalize and to what degree?


\chapter{Central Limit Theorem - how to catch a cheat}
\section{Sampling Distribution}
Generalising a conclusion from particular sample to its wider population is called statistical inference. But how is that jump made? What is hidden within a single sample that might help us infer something about its wider population? In fact, the answer lies within the study of \textbf{many} samples. Imagine drawing a random sample of fixed size from the population. It is a 'random sample' because each member of the population has an equal chance of being selected. Now record the mean \sidenote{From now on we focus on the well known sample statistic 'the mean'. This will reveal one of the mighty pillars of statistical inference.} of that sample and repeat the process of 'drawing a sample and recording the mean' a large number of times. Statisticians can't resist the urge to group these means into a theoretical construct known as the sampling distribution. It is a distribution because each of these means will differ to some degree as each sample is more or less different from the other.

As an aside, do not confuse 'sampling distribution' with a 'sample distribution': the 'sampling' describes the shape of the mean when sampling repeatedly, while the 'sample' concerns the shape of the data within a single sample.

And it is within the sampling distributions that statisticians have discovered amazing regularities which act like a bridge from the particular to the general upon which inference can pass. These regularities are captured by the 'central limit theorem'.

The central limit theorem says that if you take a large number of samples of fixed size then the distribution of those sample's means will approach a normal distribution. Further, those sample means will cluster around the mean value of the wider population. Finally, the spread of the sample means (standard deviation) will approach the population's standard deviation divided by the square root of the fixed sample size.

The theorem has been shown to be true in a wide range of circumstances and is a central engine of statistical inference. It is these regularities that connect characteristics of individual samples to population parameters.

\subsection{Catching cheats}
This regularity described by the central limit theorem can be used to detect dodgy research articles - if an RCT is performed and the data doesn't match the CLT (often over years) then something is afoot.

Here's a rather innocent title published in 2012 'The analysis of 168 randomised controlled trials to test data integrity'. 

RCT should distribute height randomly across control and treatment groups. Crass diagram showing non-random vs random distribution?

\subsection{Sample Error and Standard Error}

Earlier we explained why the mean differed between random samples. The 'sample error' or 'sample variation' (statisticians often uses the term 'error' to indicate 'noise' or 'variation' and not 'error' as in mistake) describes the degree to which a particular sample mean deviates from the population mean. Given the CLT, we expect these sample errors to vary in magnitude and resemble a normal distribution. The standard deviation of these errors is renamed 'standard error'. Therefore, the standard error gives an indication of how spread out are the magnitudes of the sample errors.

\subsection{CLT in action}
Quite often in the previous discussion we have spoken about the population parameter as though it's known which begs the question 'If you already know the population parameter - there isn't much left to infer?!'. 

Of course, we often don't know the population mean. But we can hypothesize its value and test a sample mean to see whether that value is so unusual that the hypothesized population mean looks ludicrous. We can do this because given a hypothesized mean, the CLT tells us that a distribution of the sample mean approximates a normal distribution. And it is this distribution that allows us to work out exactly how (un)likely we were to get the actual sample in our hands if the hypothesized mean were true. 





\end{document}
