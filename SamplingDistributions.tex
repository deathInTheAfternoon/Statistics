\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Sampling Distributions}
\author{Naveen Thakur }
\date{March 2021}

\begin{document}

\maketitle

\section{Introduction}
A 'sample statistic' is calculated using data from a sample. We're going to discuss how these statistics can be generalised to infer something about population parameters. To aid the discussion we focus on the simple statistic of 'the mean' as this unlocks one of statistics mighty pillars.

Let's start with a contrast. In regression analysis we take a sample and then analyse the relationship between two variables. This is very much focused upon the sample itself. However, in statistical analysis we want to go beyond talking about attributes of a single sample. We want to understand what those attributes say about the population from which the sample was drawn.

This jump from particular sample to general population is called statistical inference. But how is that jump made? What is hidden within a single sample that might help us infer something about its wider population? In fact, the answer lies within the study of \textbf{many} samples. Imagine drawing a random sample of fixed size from the population. It is a 'random sample' because each member of the population has an equal chance of being selected. Now record the mean of that sample and repeat the process of drawing a sample and recording the mean a large number of times. Statisticians group these means to form a theoretical construct known as a sampling distribution. It is a distribution because each of these means will differ to some degree as each sample is more or less different from the other.

As an aside, do not confuse 'sampling distribution' with a 'sample distribution': the 'sampling' describes the shape of the mean when sampling repeatedly, while the 'sample' concerns the shape of the data within a single sample.

And it is within the sampling distributions that statisticians have discovered amazing regularities which act like a bridge from the particular to the general upon which inference can pass. These regularities are captured by the 'central limit theorem'.

The central limit theorem says that if you take a large number of samples of fixed size then the distribution of the means will approach a normal distribution. Further, the mean of the sampling distribution approximates the mean of the population from which the samples were drawn. Finally, the standard deviation of the sampling distribution (of the sample means) is equal to the population standard deviation divided by the square root of the fixed sample size.

The theorem has been shown to be true in a wide range of circumstances and is a central engine of statistical inference. It is these regularities that connect characteristics of individual samples to population parameters.

There is some new terminology to introduce at this point. Earlier we said the mean differed between random samples. The 'sample error' or 'sample variation' (statistics often uses the term 'error' to indicate 'noise' in a process and not 'error' as in mistake) describes the degree to which a sample mean deviates from the population mean. Also, given the distribution of many sample means, their standard deviation is not called 'standard deviation' but 'standard error'.







\end{document}
